---
title: "Mistral AI"
date: 2023-12-30
---

I was able to deploy the text-generation model 'TinyLlama/TinyLlama-1.1B-Chat-v1.0' on HuggingFace resources. In the spaces files you can see the training file and training logs I developed as well for this model. Over the 3 training epochs I observed the change in perplexity, along with loss comparisons on train, validation sets over time. The positive trend was observed when training on an additional 406 data points that train and validation loss decreased over time. It was noted that if the loss values decreased, and later increased, or if the training and validation losses diverged over time, that it could be a side effect of overfitting. 

The demo is found on my HF profile: 
[FIN_LLM HF Space](https://huggingface.co/spaces/RobertCastagna/FIN_LLM)

